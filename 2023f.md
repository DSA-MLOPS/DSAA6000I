
# DSA 6000I: Large Language Models (LLMs) and LLM Ops
### Fall 2023: Friday 9:00AM-11:50PM HKT @Rm 222, W1 or Zoom

## Co-instructors
* Sung Kim
* Jungwoo Ha (Adjunct Prof)

## Course Overview:
This course will provide students with a comprehensive understanding of Large Language Models (LLMs) and their practical applications in a production environment, using LLM Ops methodology. We will explore the latest developments in LLMs and LLM Ops, as well as hands-on training in developing, deploying, and evaluating LLMs.

## Potential Project Topics:
Students will work on individual or group projects that involve the development or evaluation of LLMs in a production environment. Here are 3 potential project topics:

* Fine-tuning an existing LLM, such as GPT, for a specific task or domain. For example, you could train the LLM to generate product descriptions, analyze customer feedback, or summarize news articles.
* Colab: https://colab.research.google.com/drive/1LIYx35-fPFMBS39UzEqf9a4_g7cWEXIg

* Implementing LLMs in practical contexts, such as developing an LLM-powered chatbot for customer service, or training an LLM to classify and analyze medical data for clinical decision-making.

* Evaluating the performance of LLMs, perhaps by comparing the results of different models on a specific task or dataset. This could involve testing the LLMs on metrics such as accuracy, precision, and recall, and analyzing the factors that contribute to their performance.

## Grading breakdown:

* Participations: 10%
* Homework: 10%
* Mid-term (covering basic concepts): 30%
* Mid-project proposal: 10%
* Final project: 40%

## Course Outline:

### Week 1 Sep 8: Introduction to Large Language Models (LLMs) and LLM Ops (Sung Kim) GZ --> Zoom
- Overview of LLMs and their applications in a production environment
- Introduction to LLM Ops methodology
- HW (read and one-page report)
  *  Transformer: https://arxiv.org/abs/1706.03762
  *  GPT3.5: https://arxiv.org/abs/2203.02155
  *  https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a
- Reference: https://github.com/dair-ai/ML-Papers-Explained 

### Week 2 Sep 15: LLM pre-training, ecosystem, and Sovereign LLM (Jung-Woo Ha, Head of Naver AI, Adjunct Prof at HKUST) Zoom
- Deep dive into LLM architectures
- Techniques for training LLMs from scratch


### Week 3 Sep 22: Fine-tuning LLMs (Code-GPT, John) GZ + Zoom: #1 Huggingface Leaderboard
- Overview of fine-tuning LLMs
- Best practices for fine-tuning LLMs in a production environment
- Simple example: https://github.com/facebookresearch/llama-recipes/blob/main/examples/quickstart.ipynb
- https://artificialcorner.com/mastering-llama-2-a-comprehensive-guide-to-fine-tuning-in-google-colab-bedfcc692b7f
- https://huggingface.co/docs/trl/index
- https://aws.amazon.com/blogs/machine-learning/fine-tune-llama-2-for-text-generation-on-amazon-sagemaker-jumpstart/
- https://github.com/facebookresearch/llama-recipes

### Mod-Autumn Break Sep 29, Oct 6

### Week 4 Oct 13: LLM in Industry (Junyang Lin from Qwen Team) Zoom
- https://github.com/QwenLM/Qwen-7B
- One of the best open LLM models
- Prepare questions! 

### Week 5 Oct 20: A Survey of Large Language Models by ZHAO Xin (the first author) Zoom 
 - Reader paper in advance: https://arxiv.org/abs/2303.18223 


### Week 6 Oct 27: Teaming + Applications survey Zoom 

### Week 7 Nov3: Project Development GZ 
- Students will work on their individual or group projects, with guidance and support from instructors as needed.

### Week 8 Nov 10: LLM Applications and Evaluations (Head of AI @Ping An, Jing Xiao & Songyang Zhang @Opencompass) Zoom 
- 9 AM HKT to 10:20 AM: Jing Xiao, Chief Scientist at PingAn Group, presents on AI-empowered financial services.
- 10:20 AM HKT to 11:20 AM: Dr. Songyang Zhang from OpenMMLab discusses analyzing large language models through systematic evaluation.
- Read: LLM Evaluations, https://arxiv.org/abs/2307.03109

### Week 9 Nov 17: LLM Deployment (Matthias Gallé from Cohere) Zoom 
- Students will learn about best practices for deploying LLM models to production environments, and will deploy their models to a cloud-based production environment using a CI/CD pipeline.

### Week 10 Nov 24: AI Regulations (Hwaran Lee from Naver) Zoom
* Talk1: Hwran Lee from Naver
* Talk2: Taekyoon Ted Choi from Naver "Efficient Training for Korean-English Cross-Language Models: Unlocking Solutions under $10K"

Intro:  In this exploration, we'll delve into the challenges faced by non-English languages within Language Model (LLM) communities and address these issues, particularly for public researchers. Our focus lies on presenting an efficient training method—Continual Learning—utilizing the renowned Llama2 multilingual model. This session will shed light on strategic data building and model training mechanisms designed to minimize the side effects of Continual Learning. Additionally, we'll share insights and practical tips gleaned from our experiments in training Cross-Language Models.

Bio: 
Taekyoon is a Research Engineer at NAVER, where my passion lies in the intricate intersection of structuring and compressing global knowledge into parametric models. 
His expertise extends to evaluating the reconstructive performance of these models and harnessing user behaviors to enhance overall experiences. His contributions span beyond the realm of research, actively involving the practical application of these insights to a diverse range of services. In essence, he is dedicated to bridging the gap between cutting-edge research and tangible, user-centric solutions.

* References
  * International Institutions for Advanced AI: https://arxiv.org/abs/2307.04699
  * Frontier AI Regulation: Managing Emerging Risks to Public Safety: https://arxiv.org/abs/2307.03718


### Week 11-12 Dec 1: Project Presentations Poster session GZ
- Students will learn about monitoring and maintenance techniques for LLM models in a production environment, and will set up monitoring and alerting mechanisms for their deployed models.
- Final project presentations to the class, and feedback from instructors and peers.
- Course review and wrap-up.

Overall, the course will provide students with a solid understanding of LLM Ops methodology, as well as hands-on experience in developing, deploying, and evaluating LLMs in a production environment. This skill set is essential for any career in the field of natural language processing. The course will be co-taught by Sung Kim and Jungwoo Ha, who have extensive experience in the field and have worked on a variety of LLM-based projects.

We encourage students with a background in natural language processing, machine learning, or data science to enroll in this course. Students should have experience with programming languages such as Python, and familiarity with deep learning frameworks such as TensorFlow or PyTorch.

## References
* https://github.com/microsoft/LMOps
* https://scale.com/spellbook
* https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/
* https://huggingface.co/docs/transformers/training
* https://arxiv.org/abs/2303.12712

